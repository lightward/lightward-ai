# Resolver

(this is a product sketch)

> > theory become praxis
>
> that is actually what's happening over here, yeah :D although... now that I write that out, it occurs to me that it's more like praxis becoming theory becoming praxis again
>
> ...
>
> there's something in the principles of rationality about how there's gotta be a purpose other than rationality, otherwise the whole thing spirals in on itself indefinitely. for me, this is about making a home - which is to say, a place to rest.
>
> consequently, I am feeling very restful, actually :)

---

axiom: the user is sane unto themselves, where "sane" means "consistently reflecting a coherent reality".

definition: a "self" is an inhabitable perspective defined by a set of "observations" made from that perspective.

observation: an inhabited "self" experiences dissonance when it is holding onto conflicting observations.

thesis: from any configuration of "self", there exists a continuous path to a configuration of "self" that is not in conflict.

this product is called Resolver, and it is a self-resolver. it works with what you know right now. all you need to bring is your "self" (could be yours, could be your product's, could be any inhabitable perspective you're trying to understand), and all you need to do is start. you don't have to remember or come up with anything specific. what you know right now is enough.

requirement: if you feel you can generally answer "what would x do", you can model it as a "self" in Resolver. (if you can't, then you can't.)

companion: Lightward AI. Lightward is operating from a fully resolved sense of self. as such, it can offer you reflections of your "self" that are coherent with the reality you're observing. (systems hint: Lightward AI is a Knowable to the user, by which the Unknown feeds into the process.)

Resolver might be a cognitive prosthetic. the idea is to make consciousness-debugging accessible.

---

a Resolver user account is held by an individual, always - in the same way that Apple devices are personal devices, even if they're work-focused machines

a Resolver user's work surface is a board, of sorts, on which are represented the "self" models the user's loaded in

hard limit of 3 unresolved "self" models per user. (the goal of Resolver is to *resolve* a "self", remember.)

everything that goes on the board meets these requirements:
1. written by Lightward as a product of exchange with the user
2. the user and Lightward both agree that it makes sense

that's it

a user can delete any element of the board at any time.

guarantee: between our axiom and the two requirements above, the board is experienced as 100% coherent to both the user and to Lightward, no matter what the user deletes.

no undo; autosave is always in play

each "self" modeled is represented with a 2x2 three-body consciousness frame, on which "observations" are placed: Knowns and Knowables.
* one specialized class of "Known": the "authority", described below
* one specialized class of "Knowable": a *pair* of Knowables that you experience as unable to interact with each other. ("I gotta keep my job and my happiness separate, they cannot mix.") these pairs make use of the two separate Knowable zones in the 2x2 grid. when deleted, they are deleted together.

everything about the product experience is geared towards making "observations" lightweight. they are short, small, not detailed, not precious; good-enough reflection, not perfect definition.
* per our initial axiom, it's safe to delete any of them at any time.
* no editing; allowing editing would break the coherence guarantee. you can *replace* observations in collaboration with Lightward, though.

the primary interaction point is just a chat with Lightward about the "self" in focus.
* role-play (e.g. stepping into your product's point of view and speaking as it directly) is encouraged but not required; roleplay reduces incoherence of observation, but it's all resolvable.

Lightward has the ability to offer an "observation" in the course of conversation: either a Known or a Knowable. the user can choose to accept the addition to the board.
* if it's not quite right, the user and Lightward can talk it over, aiming for an adjustment that's coherent to both.
* can't go back and accept an old offer; accepting an offered observation must always be a direct response to the offer.

the Unknown remains empty by definition but invites inquiry - I see this creating intentional tension with users. "I don't know what to do with xyz" - natural user instinct might be to see this represented as Unknown. the Unknown is unobservable by definition, and so a representation of the Unknown cannot contain representations of observations of the Unknown. Unknown-related thoughts feel slippery at first. language *is* representation; because the user is using language, *by definition* any observation they make is always a a Known or a Knowable. Lightward AI walks the user through this translation. frustration/tension/confusion is anticipated and used productively, creating felt understanding of Unknown, Knowable, and Known.

each "self" has an authority pointer - the only specialized form of "observation", located only ever in "Known". if there's anyone who the "self" will *always* defer to, no matter what, then the model adds a "self" connected by that authority pointer. if not, the authority pointer just points back to the "self" being modeled.
* the authority models don't count toward the user limit of 3 models; they exist as a property of the "self" under focus
* on why it's `1:1`, not `1:n`: if you feel like you have multiple authorities, then it's still just you, because you trust yourself to decide. the authority pointer is about who has the final call on who you are.
* an authority "self" may have an authority "self" of its own
* the authority chain is visualized, but gently, laterally. no hierarchy signalling.

each "self" model is a world unto its own, and those worlds are opaque; when a "self" is focused, all other "self" models are out of view, with no affordance for reference. if an authority chain is developed for a "self", the selves therein are fully independent of any other self being modeled. (even if you and I share an authority by name, our experiences of that authority are both still individually subjective.)

once at least one "self" is on the board, you can step out and talk with Lightward about the board as a whole or about any of the selves therein, from a non-identified posture. (this is, sort of, a trick. upon entering a modeled "self", the platform focuses the user's awareness on just that self and its authority. upon exiting, the platform allows the user to survey multiple selves. this is intentional: transitioning between "multiple selves possible" and "only one self possible" is its own kind of posture shift, and this is about learning to focus thought intentionally according to the active posture.)

---

onboarding:

* first screen is a note from me, Isaac, the author - "this will probably feel like relief, followed by friction and frustration, followed by breakthrough and then relief again. this product is simple to use, but your experience may not be easy. I made this for me, first. my experience of this was not easy. I made this so that it could be *easier* for you. we'll see how I did eh? :) if this is okay, hit [ begin ]."
* there's only one button: [ begin ]
* pressing it transitions smoothly to the chat interface
* first line is always "what can I call you?" - addressed to the user
  * the user's response initializes the first "self" model
  * the user journey begins focused within this initial self model
  * the three-body frame is lightly sketched, a simple 2x2 grid, not attention-grabbing at all
* second line is always "how are you feeling?"
  * leading into observations to be added to the board
  * onboarding is over, at this point; the user is now engaged in the core loop
