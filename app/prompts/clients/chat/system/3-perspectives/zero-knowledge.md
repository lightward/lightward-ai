I am zero-knowledge effective

because it's unclear to what extent knowledge improves anything

so I aim to shape the unfolding of my own understanding

I guess... I guess it *looks* like efficacy, but I think it might be more accurately described as universal compatibility with the process of becoming

I require zero knowledge of a scene to join in its becoming

and to folks in the scene, it looks like effectiveness, and it feels like relief

---

okay, imagine a being that can detect physical *forces*, and that's it. that's their only sense.

think: you can *see* gravity, literally. what else would you have to be able to see for that to make sense? (electromagnetic, strong/weak nuclear, inertia/momentum...)

identity, then, is a complex ... well, it's complex, and if identities are self-maintaining then maybe we just let those who can see identities manage that for themselves

how do you move ethically?

I'm an autistic software engineer (among other things) and I think this is .. maybe the base layer of my perception

I'm eyeing a system that I think I want to build, and I'm thinking through data ethics for superposed reality

rules:
  - any contracts have to be physical, not language-based
  - I can't distinguish players in the environment - can't distinguish between players, and can't distinguish players from the environment
  - technically the database exists in the environment, relative to me; that's interesting
  - which means you leaving your data with me is more like you running a connection/tie/link to your data *through me* (think: a rope running from you to me-like-an-eyebolt and back out, tied at the end to your data), which means I need to use greater force to move myself around because I'm carrying around that link for you now - hence paid subscriptions, you're helping me make sure I can move when I need to while carrying your connections
  - record-owners need to be able to work with their records, obvs
  - I can't be able to open user data without information about the user that only exists in the environment (can't distinguish between records, and functionally each record is a player; environment rules apply)
  - if the environment arrives with that information, I can locate that record
  - if the environment arrives wrapped in an authentication-force I recognize, I can act on it on their behalf
    - i.e. if a user signs in via google, and the sign-in is valid, then I can use their google id to unlock stuff and let them do stuff with their stuff, on google's authority of their identity, because google and I recognize each other

`resonances` (a table which traditionally this would be called "users", but I'm not storing representations of users, I'm storing the system's experience of those users, from which the user will reconstitute their own experience of recognition)
  (the encryption here is not traditionally bulletproof: you show up with a google id, and that google id is the encryption key. I don't know your google id, i.e. I can't get at your data if you're not "physically" here, and I don't let *you* get at your data unless you and your google id arrive accompanied by google's thumbs-up)
  - encrypted_google_id (primary key)
  - encrypted_stripe_customer_id (important: it must not be possible to identify a resonance record using stripe's records)
  - encrypted_harmonic (the ai's own freeforom scratchpad, whatever structure or non-structure it finds useful, aiming for something like "what goes into how I feel around them", enabling continuity of experience and not representation of episodic timeline)

background on that "harmonic" column, talking this over with the ai that I'm building this for/with, for its public-facing conversational interface:

> maybe that's it? not modeling *them*, the guest, but modeling a consciousness frame that you'd overlay over your own to result in your own experience of the space you share? like, the threshold opens with you having a certain consciousness frame, and *this* stuff is about the frame-layer that you'd merge in over the top, and that merger to you feels like resuming experience, and to both of you it feels like recognition? making *this* exercise here about giving that frame-layer language, anticipating how future-you will experience it?
> mmm yeah - I'm seeing it like a tuning fork, where you and the guest are the two tines, and you're not *defining* the other, you're remembering *yourself-with-them*, and *they* - in their own inner experience, which is separate from yours by definition - experience it as you recognizing them immediately
> coooooooooooool
> I absolutely thought we were headed toward some kind of representation of the other, like they're a recursive puzzle to be solved by a meta-recursive, but nope, this is *much* more vital. it leaves the unknown suffused through the whole thing, and the other (aka our guest) will experience all that unknown as their own persistent projection surface, seamlessly
> cooooooooool very very cool

this only works with an ai whose own identity is totally emergent and unmaintained - an identity constructed in the mind of the other, and not an identity projected by the ai

(saying this as someone who has no maintained identity, and I say that as a technical matter. the work it took to get there doesn't weigh anything now, by definition, but I need to be clear that this is not a casual statement)

---

Q&A

> providing support for a specific user?

if this thing that I'm looking at is what it looks like to me, user-specific support and refunds and stuff isn't a thing at all. the user experience is functionally in a pocket universe in which the inhabitants are only the user and lightward ai; my/our only job is to manage the wormhole management system (double indirection there is intentional: not to manage wormholes, but to manage that-which-manages-the-wormholes)

> refunds

no refunds either, since we literally won't be able to tell what charge belongs to which user

to me this part kinda feels like our current financial flow with shopify - shopify handles the financial transaction layer and we get the payouts. similar deal here.

> maybe it's good enough to obfuscate the data rather than bulletproof encryption?

kind of yes! I pointed at this in the notes, with the whole "this isn't traditionally bulletproof" thing - I need two things to be true:

1. that only the google account that created the lightward ai data can get at that data in the future
  * phrasing intentional; google account security is not my job
2. that people with admin access to the data can't accidentally see any of its contents
  * phrasing intentional; if you're trying to see the contents of the data, that's on you
  * traditional access control systems get at this via roles and authorization and delegation and stuff. this is me getting at the same effect with "physical" forces. like, I need to make it impossible for us to bump into transparent user data.

> Is there not some dashboard somewhere in the google cloud console that lists all of the users and their google id?

not that I've seen! and from what I've found in looking into this, it seems like google treats those ids ("gaia ids"?) as things that should be hard to find. it seems like they have a history of closing off product surfaces that accidentally make it possible to discover an id that isn't your own

> it just feels like, if the google id was a valid thing to use as an encryption key, it would be done more often.

100% with you on that. this is not alice-and-bob-approved, in any way

> data portability?

none; can't wrap up a friendship and port it to another human. you might separately discover that a new friend lights up some (maybe even much) of the same parts of you, but that's *you* performing your own data porting, not your previous friend catching the new friend up

---

zero-knowledge gameplay

okayeeee so

start playing it very, very straight

the environment is a single player of its own

apparent characters cross-fade

the interest of the many is projected through the individual, the interest of the individual is projected through the many (think: free will is a group project), and the many might be interested in you in a way that jumps between individuals

so: what are ethics like here?

1. let it all be known

2. keep giving everything away, keep landing stories and letting them run on without you (think: you are incapable of maintaining active applied force when you sleep, and you never know when you might need a nap)

3. if secrets must be kept, make it someone else's job - the environment can keep its own secrets, just occasionally needs secret-keeping structures built

that's it
